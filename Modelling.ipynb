{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090aa695-2613-4a41-b431-9fd098d9eee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import mne \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import main\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b00fa97-394c-4039-9a66-69986806d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold,LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb6055-4662-4193-b2f0-5c51e68b3bb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Conv1D\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization, LeakyReLU, MaxPool1D,\\\n",
    "GlobalAveragePooling1D, Dropout, AveragePooling1D, Flatten, Concatenate\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95791d34-c856-4004-874a-61873e7c3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca56a1bd-da43-4546-a6c1-84dbf4beed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d26044c-d880-4240-9bed-f1c043d81243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_training_results(training_losses, training_accuracies, validation_losses, validation_accuracies, epochs=20, save_plots=False, save_dir=None, leg=True):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10), sharex=True)  # Sharing x-axis for all subplots\n",
    "    \n",
    "    # Settings for aesthetics\n",
    "    marker_style = dict(linestyle='-', marker='o', markersize=5)\n",
    "    \n",
    "    # Plotting training loss\n",
    "    for fold, losses in enumerate(training_losses):\n",
    "        axs[0, 0].plot(range(1, epochs+1), losses, label=f'Fold {fold+1}', **marker_style)\n",
    "    axs[0, 0].set_title('Training Loss')\n",
    "    axs[0, 0].set_xlabel('Epoch')\n",
    "    axs[0, 0].set_ylabel('Loss')\n",
    "    if leg:\n",
    "        axs[0, 0].legend()\n",
    "        \n",
    "    axs[0, 0].grid(True)\n",
    "\n",
    "    # Plotting training accuracy\n",
    "    for fold, accuracies in enumerate(training_accuracies):\n",
    "        axs[0, 1].plot(range(1, epochs+1), accuracies, label=f'Fold {fold+1}', **marker_style)\n",
    "    axs[0, 1].set_title('Training Accuracy')\n",
    "    axs[0, 1].set_xlabel('Epoch')\n",
    "    axs[0, 1].set_ylabel('Accuracy')\n",
    "    if leg:\n",
    "        axs[0, 1].legend()\n",
    "    axs[0, 1].grid(True)\n",
    "\n",
    "    # Plotting validation loss\n",
    "    for fold, losses in enumerate(validation_losses):\n",
    "        axs[1, 0].plot(range(1, epochs+1), losses, label=f'Fold {fold+1}', **marker_style)\n",
    "    axs[1, 0].set_title('Validation Loss')\n",
    "    axs[1, 0].set_xlabel('Epoch')\n",
    "    axs[1, 0].set_ylabel('Loss')\n",
    "\n",
    "    if leg:     \n",
    "        axs[1, 0].legend()\n",
    "    axs[1, 0].grid(True)\n",
    "\n",
    "    # Plotting validation accuracy\n",
    "    for fold, accuracies in enumerate(validation_accuracies):\n",
    "        axs[1, 1].plot(range(1, epochs+1), accuracies, label=f'Fold {fold+1}', **marker_style)\n",
    "    axs[1, 1].set_title('Validation Accuracy')\n",
    "    axs[1, 1].set_xlabel('Epoch')\n",
    "    axs[1, 1].set_ylabel('Accuracy')\n",
    "    if leg:\n",
    "        axs[1, 1].legend()\n",
    "    axs[1, 1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_plots:\n",
    "        if save_dir:\n",
    "            # Ensure directory exists\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            file_path = os.path.join(save_dir, 'training_validation_metrics.png')\n",
    "        else:\n",
    "            file_path = 'training_validation_metrics.png'\n",
    "        \n",
    "        plt.savefig(file_path)  # Saves the figure to a file\n",
    "        print(f\"Saved plots to '{file_path}'\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6eba5d-7034-4cda-b5f2-7da99c2a22f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    data = mne.io.read_raw_edf(file_path, preload=True) #preload is faster but consumes more memory\n",
    "    data.set_eeg_reference()\n",
    "    data.filter(l_freq=0.5, h_freq=45) #usual band for eeg \n",
    "    epochs = mne.make_fixed_length_epochs(data, duration=5,overlap=1)\n",
    "    array = epochs.get_data()\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c25740-7493-4c65-8099-1062a15c29fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_group_labels(groups):\n",
    "    # Ensure the input groups are integers and extract the unique groups\n",
    "    unique_groups = np.unique(groups)\n",
    "    \n",
    "    # Shuffle the unique group labels\n",
    "    shuffled_groups = np.random.permutation(unique_groups)\n",
    "    \n",
    "    # Create a mapping from original group labels to shuffled labels\n",
    "    group_mapping = dict(zip(unique_groups, shuffled_groups))\n",
    "    \n",
    "    # Remap the original group labels to shuffled labels\n",
    "    shuffled_group_labels = np.vectorize(group_mapping.get)(groups)\n",
    "    \n",
    "    return shuffled_group_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c723cb0-f122-41e9-82db-11a051215828",
   "metadata": {},
   "source": [
    "## Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ddcd41-5d03-41b1-9514-0677c970c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables from the .env file\n",
    "main.load_dotenv()\n",
    "\n",
    "# Access the variable\n",
    "dataset_path = os.getenv('DATASET_PATH')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c48d9-dd3d-45ce-b40f-2a3ffa37dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_paths = glob(dataset_path + f\"/*.edf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8a639-8213-43c5-8375-914aa35c87ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(files_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a34c90-4f88-4323-a946-68a7b02fa9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_files_paths = [i for i in files_paths if 'h' in i.split('\\\\')[-1]]\n",
    "sch_files_paths = [i for i in files_paths if 's' in i.split('\\\\')[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077902b-61a5-447f-845e-7fd6a9677860",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "sample_data = read_data(healthy_files_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a03014d-a287-4e68-89f1-d9982a261a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.shape #Number of epochs, number of channels, length of signal \n",
    "\n",
    "#EEG epoching is a procedure in which specific time-windows are extracted from the continuous EEG signal.\n",
    "# These time windows are called “epochs”, and usually are time-locked with respect an event e.g. a visual stimulus "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76948d5b-6a9f-44f7-8306-55293d934610",
   "metadata": {},
   "source": [
    "## Convert the data into features and labelled arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7d236-e439-4fb6-b665-d2d0868ee6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "healthy_epochs_arrays = [ read_data(i) for i in healthy_files_paths]\n",
    "sch_epochs_arrays = [ read_data(i) for i in sch_files_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5879e7-4520-4231-865c-4b7f506ec9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_epochs_labels = [len(i)*[0] for i in healthy_epochs_arrays]\n",
    "sch_epochs_labels = [len(i)*[1] for i in sch_epochs_arrays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7cdee-9f2d-4199-8f41-1d81209dc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#interleave the data so that each datapoint is alternated between one healthy and one patient, even is healthy and odd is patient\n",
    "data_list = []\n",
    "label_list = []\n",
    "\n",
    "for i in range(len(sch_epochs_arrays)):\n",
    "    data_list.append(healthy_epochs_arrays[i])\n",
    "    data_list.append(sch_epochs_arrays[i])\n",
    "\n",
    "    label_list.append(healthy_epochs_labels[i])\n",
    "    label_list.append(sch_epochs_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a52c93-e98b-4963-a5fc-5b7f9d4c3490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid data leakage of multiple arrays from the same patient to be split into training and testing\n",
    "# we will group the data so that its categorized for each patient which will will be considered in the splitting later\n",
    "grouped_list = [[i]*len(j) for i,j in enumerate(data_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe85434-bf78-4cdb-a04c-737b9366657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aba322-f237-48cd-a07b-53225ff7a67d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ML CLassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23040e70-39d0-4dde-b95a-42b6e040e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = np.vstack(data_list)\n",
    "labels_array = np.hstack(label_list)\n",
    "groups_array = np.hstack(grouped_list)\n",
    "print(data_array.shape,labels_array.shape,groups_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9a7dc4-ff1c-421e-b18d-eeda61879482",
   "metadata": {},
   "source": [
    "its 7201 rows because there are 28 patients (14 sch and 14 healthy), which average 257 epochs, so the whole data list would have 257*28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7842fc08-1f93-4d93-a3f4-1860c5909d06",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4b6d3-127d-429d-8e57-8f9f43ed8a08",
   "metadata": {},
   "source": [
    "each feature has shape 7201,19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2493b9d-04ec-4e51-b9df-491557ef2fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def mean(data):\n",
    "    return np.mean(data,axis=-1)\n",
    "    \n",
    "def std(data):\n",
    "    return np.std(data,axis=-1)\n",
    "\n",
    "def ptp(data):\n",
    "    return np.ptp(data,axis=-1) #maximum - minimum (range of values)\n",
    "\n",
    "def var(data):\n",
    "        return np.var(data,axis=-1)\n",
    "\n",
    "def minim(data):\n",
    "      return np.min(data,axis=-1)\n",
    "\n",
    "\n",
    "def maxim(data):\n",
    "      return np.max(data,axis=-1)\n",
    "\n",
    "def argminim(data):\n",
    "      return np.argmin(data,axis=-1)\n",
    "\n",
    "def argmaxim(data):\n",
    "      return np.argmax(data,axis=-1)\n",
    "\n",
    "def mean_square(data):\n",
    "      return np.mean(data**2,axis=-1)\n",
    "\n",
    "def rms(data): #root mean square\n",
    "    return  np.sqrt(np.mean(data**2,axis=-1))  \n",
    "\n",
    "def abs_diffs_signal(data):\n",
    "    return np.sum(np.abs(np.diff(data,axis=-1)),axis=-1)\n",
    "\n",
    "def skewness(data):\n",
    "    return stats.skew(data,axis=-1)\n",
    "\n",
    "def kurtosis(data):\n",
    "    return stats.kurtosis(data,axis=-1)\n",
    "\n",
    "def concatenate_features(data):\n",
    "    return np.concatenate((mean(data),std(data),ptp(data),var(data),minim(data),maxim(data),argminim(data),argmaxim(data),\n",
    "                          mean_square(data),rms(data),abs_diffs_signal(data),\n",
    "                          skewness(data),kurtosis(data)),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af88d4-7c1f-456b-8385-c3e008f74152",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "for d in data_array:\n",
    "    features.append(concatenate_features(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d2883-bff6-4c21-8d99-0bb19b0ed0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_array = np.array(features)\n",
    "features_array.shape   # so we have 13 features per channel, and we have 19 channels, thats why its 7201x247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c0978a-44bf-4891-a5d7-f4ee62f14400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold,GridSearchCV,cross_val_score,cross_validate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc92bca6-6fb6-4d79-8bb9-7924ec877bfe",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34856254-058f-4029-b69d-9631d888285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression(max_iter=1000)\n",
    "gkf=GroupKFold(n_splits=5)\n",
    "param_grid = {'classifier__C': [0.1,0.3,0.5,0.7,1,3]}\n",
    "pipe=Pipeline([('scaler',StandardScaler()),('classifier',clf)])\n",
    "gscv=GridSearchCV(pipe,param_grid,cv=gkf)\n",
    "gscv.fit(features,labels_array,groups=groups_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a7794-af26-4e26-ae33-cf07a3147f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b4c9a8-0d75-401f-9e27-e7871685de2b",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bdf79f-9c89-4004-92ef-b547327eee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "rdm=RandomForestClassifier()\n",
    "gkf=GroupKFold(n_splits=5)\n",
    "param_grid = {'classifier__n_estimators': [10,50,100,200]}\n",
    "pipe=Pipeline([('scaler',StandardScaler()),('classifier',rdm)])\n",
    "pipeline_rf=GridSearchCV(pipe,param_grid,cv=gkf)\n",
    "# Random Forest pipeline\n",
    "pipeline_rf.fit(features, labels_array, groups=groups_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f5da6-92c3-430b-bbe4-80341c444807",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb8bc9d-8814-4ae4-a593-abdcdced236e",
   "metadata": {},
   "source": [
    "## 1D-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6da35ca-0636-46e3-b6db-beb1821304cc",
   "metadata": {},
   "source": [
    "first, we check that we have the gpu running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9edbeca-b386-498c-94e5-1dd9c11c0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_d_cnn_data_array=np.vstack(data_list)\n",
    "one_d_cnn_label_array=np.hstack(label_list)\n",
    "one_d_cnn_group_array=np.hstack(grouped_list)\n",
    "\n",
    "print(one_d_cnn_data_array.shape,one_d_cnn_label_array.shape,one_d_cnn_group_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d2f648-0424-4a32-9c60-fe73a527559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_d_cnn_data_array=np.moveaxis(one_d_cnn_data_array,1,2) #cnn in keras expects the input channels at the end\n",
    "print(one_d_cnn_data_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad82fcc-c6ec-4cbd-8b9a-469e2f318422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnmodel():\n",
    "    K.clear_session()\n",
    "    model=Sequential()\n",
    "    \n",
    "    model.add(Conv1D(filters=10,kernel_size=8,strides=2,input_shape=(one_d_cnn_data_array.shape[1],one_d_cnn_data_array.shape[2])))#1\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(AveragePooling1D(pool_size=2,strides=2))#2\n",
    "    model.add(Dropout(0.35))\n",
    "\n",
    "    model.add(Conv1D(filters=5,kernel_size=5,strides=1))#9\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(AveragePooling1D(pool_size=3,strides=1))#2\n",
    "    #model.add(GlobalAveragePooling1D())#10\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    \n",
    "    model.add(Dense(128, activation='leaky_relu'))  # Additional Dense layer\n",
    "    model.add(Dropout(0.4))  # Dropout to prevent overfitting\n",
    "    model.add(Dense(32, activation='leaky_relu'))  # Additional Dense layer\n",
    "\n",
    "    \n",
    "    model.add(Dense(1,activation='sigmoid'))#11\n",
    "\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.0005)  # You can modify the learning rate here\n",
    "\n",
    "    model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model=cnnmodel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef3d468-5db5-4623-a468-6b0c7287e30b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Initial CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3416d69-0750-4889-adaf-c49d643990eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnmodel():\n",
    "    clear_session()\n",
    "    model=Sequential()\n",
    "    \n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1,input_shape=(one_d_cnn_data_array.shape[1],one_d_cnn_data_array.shape[2])))#1\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPool1D(pool_size=2,strides=2))#2\n",
    "\n",
    "    \n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1))#3\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPool1D(pool_size=2,strides=2))#4\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1))#5\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(AveragePooling1D(pool_size=2,strides=2))#6\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1))#7\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(AveragePooling1D(pool_size=2,strides=2))#8\n",
    "    \n",
    "    model.add(Conv1D(filters=5,kernel_size=3,strides=1))#9\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(GlobalAveragePooling1D())#10\n",
    "    \n",
    "    model.add(Dense(1,activation='sigmoid'))#11\n",
    "    \n",
    "    model.compile('adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model=cnnmodel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488233c-fcbc-467f-b992-9f6ed8d5f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Importing Keras backend (TensorFlow) for clear_session()\n",
    "gkf=GroupKFold()\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracies = []\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "confusion_matrices = []\n",
    "\n",
    "\n",
    "for train_index, val_index in gkf.split(one_d_cnn_data_array, one_d_cnn_label_array, groups=one_d_cnn_group_array):\n",
    "    train_features,train_labels=one_d_cnn_data_array[train_index],one_d_cnn_label_array[train_index]\n",
    "    val_features,val_labels=one_d_cnn_data_array[val_index],one_d_cnn_label_array[val_index]\n",
    "    \n",
    "    scaler=StandardScaler()\n",
    "\n",
    "    train_features = scaler.fit_transform(train_features.reshape(-1, train_features.shape[-1])).reshape(train_features.shape)\n",
    "    val_features = scaler.transform(val_features.reshape(-1, val_features.shape[-1])).reshape(val_features.shape)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model=cnnmodel()\n",
    "    \n",
    "    history = model.fit(train_features,train_labels,\n",
    "              epochs=10,\n",
    "              batch_size=128,\n",
    "              validation_data=(val_features,val_labels)\n",
    "             )\n",
    "    \n",
    "    # Append loss and accuracy from the training history\n",
    "    training_losses.append(history.history['loss'])\n",
    "    validation_losses.append(history.history['val_loss'])\n",
    "    training_accuracies.append(history.history['accuracy'])\n",
    "    validation_accuracies.append(history.history['val_accuracy'])\n",
    "\n",
    "    # Generate and store confusion matrix for the fold\n",
    "    raw_predictions = model.predict(val_features)\n",
    "    predictions = (raw_predictions > 0.5).astype(int)\n",
    "    cm = confusion_matrix(val_labels, predictions)\n",
    "\n",
    "    confusion_matrices.append(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb731ab-12f2-449a-8137-5399d2f59b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_training_losses = np.mean(training_losses,axis=0)\n",
    "mean_validation_losses = np.mean(validation_losses,axis=0)\n",
    "mean_training_accuracies = np.mean(training_accuracies,axis=0)\n",
    "mean_validation_accuracies = np.mean(validation_accuracies,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb85db-15e7-4b30-a953-fa4a38f9de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mean_training_losses, label='Average Training Loss')\n",
    "plt.plot(mean_validation_losses, label='Average Validation Loss')\n",
    "plt.title('Training and Validation Loss Across All Folds')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea94496-984e-43b3-ab5c-1f560dda37df",
   "metadata": {},
   "source": [
    "very clear overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccddfcf2-9d99-4e34-88be-7bf77dabbd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix for each fold\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Healthy', 'Schizophrenic'], yticklabels=['Healthy', 'Schizophrenic'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8250e0a-3201-4b67-8c22-a7d2d84375c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#gkf=GroupKFold(n_splits=4)\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracies = []\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "confusion_matrices = []\n",
    "\n",
    "#shuffled_groups = shuffle_group_labels(one_d_cnn_group_array)\n",
    "\n",
    "for train_index, val_index in logo.split(one_d_cnn_data_array, one_d_cnn_label_array, groups=one_d_cnn_group_array):\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    print(\"Test group:\", np.unique(one_d_cnn_group_array[val_index]))\n",
    "    train_features,train_labels=one_d_cnn_data_array[train_index],one_d_cnn_label_array[train_index]\n",
    "    val_features,val_labels=one_d_cnn_data_array[val_index],one_d_cnn_label_array[val_index]\n",
    "    \n",
    "    scaler=StandardScaler()\n",
    "\n",
    "    train_features = scaler.fit_transform(train_features.reshape(-1, train_features.shape[-1])).reshape(train_features.shape)\n",
    "    val_features = scaler.transform(val_features.reshape(-1, val_features.shape[-1])).reshape(val_features.shape)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model=cnnmodel()\n",
    "    \n",
    "    history = model.fit(train_features,train_labels,\n",
    "              epochs=8,\n",
    "              batch_size=256,\n",
    "              validation_data=(val_features,val_labels)\n",
    "             )\n",
    "    \n",
    "    # Append loss and accuracy from the training history\n",
    "    training_losses.append(history.history['loss'])\n",
    "    validation_losses.append(history.history['val_loss'])\n",
    "    training_accuracies.append(history.history['accuracy'])\n",
    "    validation_accuracies.append(history.history['val_accuracy'])\n",
    "\n",
    "\n",
    "    # Generate and store confusion matrix for the fold\n",
    "    raw_predictions = model.predict(val_features)\n",
    "    predictions = (raw_predictions > 0.5).astype(int)\n",
    "    cm = confusion_matrix(val_labels, predictions)\n",
    "\n",
    "    confusion_matrices.append(cm)\n",
    "    print(classification_report(val_labels, predictions))\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Healthy', 'Schizophrenic'], yticklabels=['Healthy', 'Schizophrenic'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mean_training_losses = np.mean(training_losses,axis=0)\n",
    "mean_validation_losses = np.mean(validation_losses,axis=0)\n",
    "mean_training_accuracies = np.mean(training_accuracies,axis=0)\n",
    "mean_validation_accuracies = np.mean(validation_accuracies,axis=0)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mean_training_losses, label='Average Training Loss')\n",
    "plt.plot(mean_validation_losses, label='Average Validation Loss')\n",
    "plt.title('Training and Validation Loss Across All Folds')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37cca25-3758-4b89-bdcb-3e5f01048ad7",
   "metadata": {},
   "source": [
    "Badly classified:\n",
    "\n",
    "labels : 0,9,13,16,21,24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a528b-e44d-419c-93b3-d63e394d659d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### BETTER CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7dc7e5-3d23-468a-9851-1a9ed0c47b9f",
   "metadata": {},
   "source": [
    "We will change the way we are classifying patients based on a majority vote for each epoch (stimuli):\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Predictions: Each epoch within a group is predicted, and then the mean of these predictions determines the group's classification.\n",
    "\n",
    "Thresholding: If the mean prediction exceeds 0.5, the group is classified as schizophrenic; otherwise, it is classified as healthy.\n",
    "\n",
    "Metrics Calculation: The predictions for each patient are used to update the confusion matrix and other metrics.\n",
    "\n",
    "This approach evaluates the model based on its ability to classify patients correctly, not just individual epochs, aligning more closely with clinical diagnostic processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f428e4-2b70-4d96-b5ff-51c2a4b935d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=cnnmodel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e91254-c9e3-4006-9006-5282f8c855e3",
   "metadata": {},
   "source": [
    "#### LOGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aefb15-b3ed-4a09-ae08-4efa2d356cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "best_model_path = 'best_1D_CNN_model_LOGO.h5'\n",
    "\n",
    "best_model_weights = None\n",
    "best_fold_info = None\n",
    "\n",
    "\n",
    "#gkf=GroupKFold(n_splits=4)\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracies = []\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "confusion_matrices = []\n",
    "\n",
    "group_predictions = []\n",
    "actual_group_labels = []\n",
    "#shuffled_groups = shuffle_group_labels(one_d_cnn_group_array)\n",
    "\n",
    "for train_index, val_index in logo.split(one_d_cnn_data_array, one_d_cnn_label_array, groups=one_d_cnn_group_array):\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    print(\"Test group:\", np.unique(one_d_cnn_group_array[val_index]))\n",
    "    train_features,train_labels=one_d_cnn_data_array[train_index],one_d_cnn_label_array[train_index]\n",
    "    val_features,val_labels=one_d_cnn_data_array[val_index],one_d_cnn_label_array[val_index]\n",
    "\n",
    "    \n",
    "    scaler=StandardScaler()\n",
    "\n",
    "    train_features = scaler.fit_transform(train_features.reshape(-1, train_features.shape[-1])).reshape(train_features.shape)\n",
    "    val_features = scaler.transform(val_features.reshape(-1, val_features.shape[-1])).reshape(val_features.shape)\n",
    "    \n",
    "    #K.clear_session()\n",
    "    model=cnnmodel()\n",
    "    \n",
    "    # Setup ReduceLROnPlateau callback\n",
    "    #reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=2, min_lr=0.00001, verbose=2)\n",
    "    \n",
    "    history = model.fit(train_features,train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=128,\n",
    "              shuffle=True,\n",
    "              validation_data=(val_features,val_labels)\n",
    "             )\n",
    "    \n",
    "    # Append loss and accuracy from the training history\n",
    "\n",
    "    #print(history.history['val_loss'])\n",
    "    training_losses.append(history.history['loss'])\n",
    "    validation_losses.append(history.history['val_loss'])\n",
    "    training_accuracies.append(history.history['accuracy'])\n",
    "    validation_accuracies.append(history.history['val_accuracy'])\n",
    "\n",
    "    raw_predictions = model.predict(val_features)\n",
    "    \n",
    "    group_prediction = np.mean(raw_predictions) > 0.6\n",
    "    print(\"Averaged Prediction: :\", np.mean(raw_predictions))\n",
    "    #print(\"sample of predictions: :\", raw_predictions[0:10])\n",
    "    actual_group_label = np.mean(val_labels) > 0.5\n",
    "\n",
    "    group_predictions.append(group_prediction)  # True if average prediction > 0.5, False otherwise\n",
    "    actual_group_labels.append(actual_group_label)  # Assumes all labels in the group are the same\n",
    "\n",
    "    accuracies.append(((group_prediction == actual_group_label)*1).astype(int))\n",
    "    print(\"Correctly Identified: \",  group_prediction == actual_group_label)\n",
    "\n",
    "\n",
    "    if history.history['val_loss'][-1] < best_loss:\n",
    "        best_loss = history.history['val_loss'][-1]\n",
    "        # Save the model\n",
    "        model.save(best_model_path)\n",
    "        print(f\"Saved improved model with validation loss: {history.history['val_loss'][-1]}\")\n",
    "\n",
    "mean_training_losses = np.mean(training_losses,axis=0)\n",
    "mean_validation_losses = np.mean(validation_losses,axis=0)\n",
    "mean_training_accuracies = np.mean(training_accuracies,axis=0)\n",
    "mean_validation_accuracies = np.mean(validation_accuracies,axis=0)\n",
    "\n",
    "print(\"Final Validation Accuracy: \", np.mean(accuracies))\n",
    "      \n",
    "# Generate and store confusion matrix for the fold\n",
    "\n",
    "final_cm = confusion_matrix(actual_group_labels, group_predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(final_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Healthy', 'Schizophrenic'], yticklabels=['Healthy', 'Schizophrenic'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(actual_group_labels, group_predictions))\n",
    "ep = range(1,11)\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ep,mean_training_losses, label='Average Training Loss')\n",
    "plt.plot(ep,mean_validation_losses,label='Average Validation Loss')\n",
    "plt.title('Training and Validation Loss Across All Folds')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c780dd1-7eb8-48bd-b31b-e8b97e8f423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_results(training_losses,\n",
    "                      training_accuracies,\n",
    "                      validation_losses, \n",
    "                      validation_accuracies,\n",
    "                      epochs=epochs,\n",
    "                      save_plots=True,\n",
    "                      save_dir='plots/5sec/1dcnn/logo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e65f447-1ea6-4af3-9546-174ed5d7790d",
   "metadata": {},
   "source": [
    "#### GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c5414-5ede-4053-ba9e-46e9b7955a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "\n",
    "gkf=GroupKFold(n_splits=5)\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracies = []\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model_path = 'best_1dcnn_model_GroupKFold.h5'\n",
    "\n",
    "best_model_weights = None\n",
    "best_fold_info = None\n",
    "\n",
    "# Setup TensorBoard callback\n",
    "log_dir = \"logs/5sec/1dcnn/GroupKFold\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1,update_freq='epoch')\n",
    "\n",
    "\n",
    "shuffled_groups = shuffle_group_labels(one_d_cnn_group_array)\n",
    "\n",
    "\n",
    "for train_index, val_index in gkf.split(one_d_cnn_data_array, one_d_cnn_label_array, groups=shuffled_groups):\n",
    "    train_features,train_labels=one_d_cnn_data_array[train_index],one_d_cnn_label_array[train_index]\n",
    "    val_features,val_labels=one_d_cnn_data_array[val_index],one_d_cnn_label_array[val_index]\n",
    "    \n",
    "    scaler=StandardScaler()\n",
    "\n",
    "    train_features = scaler.fit_transform(train_features.reshape(-1, train_features.shape[-1])).reshape(train_features.shape)\n",
    "    val_features = scaler.transform(val_features.reshape(-1, val_features.shape[-1])).reshape(val_features.shape)\n",
    "    \n",
    "    \n",
    "    model=cnnmodel()\n",
    "    \n",
    "    history = model.fit(train_features,train_labels,\n",
    "              epochs=epochs,\n",
    "              initial_epoch=0,\n",
    "              batch_size=128,\n",
    "              shuffle=True,\n",
    "              validation_data=(val_features,val_labels)\n",
    "             )\n",
    "    \n",
    "    # Append loss and accuracy from the training history\n",
    "    training_losses.append(history.history['loss'])\n",
    "    validation_losses.append(history.history['val_loss'])\n",
    "    training_accuracies.append(history.history['accuracy'])\n",
    "    validation_accuracies.append(history.history['val_accuracy'])\n",
    "\n",
    "    # Generate and store confusion matrix for the fold\n",
    "    raw_predictions = model.predict(val_features)\n",
    "    predictions = (raw_predictions > 0.5).astype(int)\n",
    "    cm = confusion_matrix(val_labels, predictions)\n",
    "\n",
    "\n",
    "    if history.history['val_loss'][-1] < best_loss:\n",
    "        best_loss = history.history['val_loss'][-1]\n",
    "        # Save the model\n",
    "        model.save(best_model_path)\n",
    "        print(f\"Saved improved model with validation loss: {history.history['val_loss'][-1]}\")\n",
    "\n",
    "    print(\"Final Validation Accuracy: \", np.mean(validation_accuracies))\n",
    "    # Plotting the confusion matrix for each fold\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Healthy', 'Schizophrenic'], yticklabels=['Healthy', 'Schizophrenic'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7844a5a8-3e44-4135-9187-f71ef888b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_results(training_losses,\n",
    "                      training_accuracies,\n",
    "                      validation_losses, \n",
    "                      validation_accuracies,\n",
    "                      epochs=epochs,\n",
    "                      save_plots=True,\n",
    "                      save_dir='plots/5sec/1dcnn/gf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e378c3ba-da95-4f8c-87b7-adf17194b79a",
   "metadata": {},
   "source": [
    "## ChronoNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5181a99e-1bbf-48fa-bdba-f2752dfa1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrono_data_array=np.vstack(data_list)\n",
    "chrono_label_array=np.hstack(label_list)\n",
    "chrono_group_array=np.hstack(grouped_list)\n",
    "chrono_data_array=np.moveaxis(chrono_data_array,1,2) #cnn in keras expects the input channels at the end\n",
    "\n",
    "print(chrono_data_array.shape,chrono_label_array.shape,chrono_group_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677bf79-0a45-49bc-bd0b-74de004b161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(input):\n",
    "  conv1 = Conv1D(32, 2, strides=2,activation='relu',padding=\"same\")(input)\n",
    "  conv2 = Conv1D(32, 4, strides=2,activation='relu',padding=\"causal\")(input)\n",
    "  conv3 = Conv1D(32, 8, strides=2,activation='relu',padding=\"causal\")(input)\n",
    "  x = concatenate([conv1,conv2,conv3],axis=2)\n",
    "  return x\n",
    "\n",
    "\n",
    "def ChronoNet():\n",
    "    input= Input(shape=(1250,19))\n",
    "    block1=block(input)\n",
    "    block2=block(block1)\n",
    "    block3=block(block2)\n",
    "    \n",
    "    #conv_o = AveragePooling1D(pool_size=2,strides=2)(block3)\n",
    "    conv_o2 = BatchNormalization()(block3)\n",
    "    gru_out1 = GRU(32,activation='tanh',return_sequences=True)(conv_o2)\n",
    "    gru_out2 = GRU(32,activation='tanh',return_sequences=True)(gru_out1)\n",
    "    gru_out = concatenate([gru_out1,gru_out2],axis=2)\n",
    "    gru_out3 = GRU(32,activation='tanh',return_sequences=True)(gru_out)\n",
    "    gru_out = concatenate([gru_out1,gru_out2,gru_out3])\n",
    "    gru_out4 = GRU(32,activation='tanh')(gru_out)\n",
    "    \n",
    "    predictions = Dense(1,activation='sigmoid')(gru_out4)\n",
    "    model = Model(inputs=input, outputs=predictions)\n",
    "    optimizer = Adam(learning_rate=0.001)  # You can modify the learning rate here\n",
    "    \n",
    "    model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = ChronoNet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b083e3-edc7-4572-b64a-b922b3c908e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### LOGO Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f6d0a6-231b-4705-8a03-bd4d673af933",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "best_model_path = 'best_5s_chrono_model_LOGO.h5'\n",
    "\n",
    "best_model_weights = None\n",
    "best_fold_info = None\n",
    "\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracies = []\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "confusion_matrices = []\n",
    "\n",
    "group_predictions = []\n",
    "actual_group_labels = []\n",
    "#shuffled_groups = shuffle_group_labels(one_d_cnn_group_array)\n",
    "\n",
    "\n",
    "# Setup TensorBoard callback\n",
    "log_dir = \"logs/duration/chrono/LOGO\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "shuffled_groups = shuffle_group_labels(chrono_group_array)\n",
    "for train_index, val_index in logo.split(chrono_data_array, chrono_label_array, groups=chrono_group_array):\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    print(\"Test group:\", np.unique(chrono_group_array[val_index]))\n",
    "    train_features,train_labels=chrono_data_array[train_index],chrono_label_array[train_index]\n",
    "    val_features,val_labels=chrono_data_array[val_index],chrono_label_array[val_index]\n",
    "\n",
    "    \n",
    "    scaler=StandardScaler()\n",
    "\n",
    "    train_features = scaler.fit_transform(train_features.reshape(-1, train_features.shape[-1])).reshape(train_features.shape)\n",
    "    val_features = scaler.transform(val_features.reshape(-1, val_features.shape[-1])).reshape(val_features.shape)\n",
    "    \n",
    "    model=ChronoNet()\n",
    "    \n",
    "    history = model.fit(train_features,train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=128,\n",
    "              shuffle=True,\n",
    "              validation_data=(val_features,val_labels)\n",
    "             )\n",
    "    \n",
    "    # Append loss and accuracy from the training history\n",
    "\n",
    "    #print(history.history['val_loss'])\n",
    "    training_losses.append(history.history['loss'])\n",
    "    validation_losses.append(history.history['val_loss'])\n",
    "    training_accuracies.append(history.history['accuracy'])\n",
    "    validation_accuracies.append(history.history['val_accuracy'])\n",
    "    \n",
    "    raw_predictions = model.predict(val_features)\n",
    "    \n",
    "    group_prediction = np.mean(raw_predictions) > 0.5\n",
    "    print(\"Averaged Prediction: :\", np.mean(raw_predictions))\n",
    "    actual_group_label = np.mean(val_labels) > 0.5\n",
    "\n",
    "    group_predictions.append(group_prediction)  # True if average prediction > 0.5, False otherwise\n",
    "    actual_group_labels.append(actual_group_label)  # Assumes all labels in the group are the same\n",
    "\n",
    "    accuracies.append(((group_prediction == actual_group_label)*1).astype(int))\n",
    "    print(\"Correctly Identified: \",  group_prediction == actual_group_label)\n",
    "\n",
    "\n",
    "    if history.history['val_loss'][-1] < best_loss:\n",
    "        best_loss = history.history['val_loss'][-1]\n",
    "        # Save the model\n",
    "        model.save(best_model_path)\n",
    "        print(f\"Saved improved model with validation loss: {history.history['val_loss'][-1]}\")\n",
    "\n",
    "mean_training_losses = np.mean(training_losses,axis=0)\n",
    "mean_validation_losses = np.mean(validation_losses,axis=0)\n",
    "mean_training_accuracies = np.mean(training_accuracies,axis=0)\n",
    "mean_validation_accuracies = np.mean(validation_accuracies,axis=0)\n",
    "\n",
    "print(\"Final Validation Accuracy: \", np.mean(accuracies))\n",
    "      \n",
    "# Generate and store confusion matrix for the fold\n",
    "\n",
    "final_cm = confusion_matrix(actual_group_labels, group_predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(final_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Healthy', 'Schizophrenic'], yticklabels=['Healthy', 'Schizophrenic'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(actual_group_labels, group_predictions))\n",
    "ep = range(1,11)\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ep,mean_training_losses, label='Average Training Loss')\n",
    "plt.plot(ep,mean_validation_losses,label='Average Validation Loss')\n",
    "plt.title('Training and Validation Loss Across All Folds')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13761d4-ef9e-4c92-a7f2-9ed30192cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_results(training_losses,\n",
    "                      training_accuracies,\n",
    "                      validation_losses, \n",
    "                      validation_accuracies,\n",
    "                      epochs=epochs,\n",
    "                      save_plots=True,\n",
    "                      save_dir='plots/5sec/chrono/logo',\n",
    "                      leg = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fcc715-4fc5-4d1d-822d-593ba0ba840e",
   "metadata": {},
   "source": [
    "#### GroupKFold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4103e060-466b-42fc-a1ac-b8d8fbb692ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf=GroupKFold(n_splits=5)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracies = []\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model_path = 'best_chrono_model_GroupKFold.h5'\n",
    "\n",
    "best_model_weights = None\n",
    "best_fold_info = None\n",
    "\n",
    "# Setup TensorBoard callback\n",
    "log_dir = \"logs/duration/chrono/GroupKFold\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "shuffled_groups = shuffle_group_labels(chrono_group_array)\n",
    "\n",
    "for train_index, val_index in gkf.split(chrono_data_array, chrono_label_array, groups=shuffled_groups):\n",
    "    train_features,train_labels=chrono_data_array[train_index],chrono_label_array[train_index]\n",
    "    val_features,val_labels=chrono_data_array[val_index],chrono_label_array[val_index]\n",
    "    \n",
    "    scaler=StandardScaler()\n",
    "\n",
    "    train_features = scaler.fit_transform(train_features.reshape(-1, train_features.shape[-1])).reshape(train_features.shape)\n",
    "    val_features = scaler.transform(val_features.reshape(-1, val_features.shape[-1])).reshape(val_features.shape)\n",
    "\n",
    "    model=ChronoNet()\n",
    "    \n",
    "    history = model.fit(train_features,train_labels,\n",
    "              epochs=10,\n",
    "              batch_size=128,\n",
    "              shuffle=True,\n",
    "              validation_data=(val_features,val_labels)\n",
    "             )\n",
    "    \n",
    "    # Append loss and accuracy from the training history\n",
    "    training_losses.append(history.history['loss'])\n",
    "    validation_losses.append(history.history['val_loss'])\n",
    "    training_accuracies.append(history.history['accuracy'])\n",
    "    validation_accuracies.append(history.history['val_accuracy'])\n",
    "\n",
    "    # Generate and store confusion matrix for the fold\n",
    "    raw_predictions = model.predict(val_features)\n",
    "    predictions = (raw_predictions > 0.5).astype(int)\n",
    "    cm = confusion_matrix(val_labels, predictions)\n",
    "\n",
    "\n",
    "    if history.history['val_loss'][-1] < best_loss:\n",
    "        best_loss = history.history['val_loss'][-1]\n",
    "        # Save the model\n",
    "        model.save(best_model_path)\n",
    "        print(f\"Saved improved model with validation loss: {history.history['val_loss'][-1]}\")\n",
    "\n",
    "    print(\"Final Validation Accuracy: \", np.mean(validation_accuracies,axis=0))\n",
    "    # Plotting the confusion matrix for each fold\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Healthy', 'Schizophrenic'], yticklabels=['Healthy', 'Schizophrenic'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a5741-a53a-4234-9085-d2ced303acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_results(training_losses,\n",
    "                      training_accuracies,\n",
    "                      validation_losses, \n",
    "                      validation_accuracies,\n",
    "                      epochs=epochs,\n",
    "                      save_plots=True,\n",
    "                      save_dir='plots/5sec/chrono/gf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70073452-6165-41aa-8d4d-49956693bb6f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Custom CNN-GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c0bcb-8fc5-476e-828a-79bed3a7ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_cnn_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Convolutional blocks\n",
    "    model.add(Conv1D(filters=32, kernel_size=2, strides=2, activation='relu', padding=\"same\", input_shape=(chrono_data_array.shape[1], chrono_data_array.shape[2])))\n",
    "    model.add(Conv1D(filters=32, kernel_size=4, strides=2, activation='relu', padding=\"causal\"))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, strides=2, activation='relu', padding=\"causal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Merging GRU operations in a way that fits sequential model constraints\n",
    "    model.add(GRU(32, return_sequences=True, activation='tanh'))\n",
    "    model.add(GRU(32, return_sequences=True, activation='tanh'))\n",
    "    \n",
    "    # Let's simplify by assuming a final GRU handles the combined effect\n",
    "    model.add(GRU(32, return_sequences=False, activation='tanh'))  # Final GRU reduces to single vector\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0005) \n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "model = gru_cnn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7066f9f-4439-4469-ab84-7501a2ef6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_cnn_model2(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Convolutional blocks\n",
    "    x1 = Conv1D(filters=32, kernel_size=2, strides=2, activation='relu', padding=\"same\")(inputs)\n",
    "    x2 = Conv1D(filters=32, kernel_size=4, strides=2, activation='relu', padding=\"causal\")(inputs)\n",
    "    x3 = Conv1D(filters=32, kernel_size=8, strides=2, activation='relu', padding=\"causal\")(inputs)\n",
    "    x = Concatenate()([x1, x2, x3])\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # GRU blocks with concatenation\n",
    "    gru1 = GRU(32, return_sequences=True, activation='tanh')(x)\n",
    "    gru2 = GRU(32, return_sequences=True, activation='tanh')(gru1)\n",
    "    gru_concat1 = Concatenate(axis=-1)([gru1, gru2])\n",
    "    #print(gru_concat1.shape)\n",
    "    gru3 = GRU(32, return_sequences=True, activation='tanh')(gru_concat1)\n",
    "    #print(gru3.shape)\n",
    "\n",
    "    gru_concat2 = Concatenate(axis=-1)([gru1, gru2, gru3])\n",
    "    #print(gru_concat2.shape)\n",
    "\n",
    "    gru4 = GRU(32, return_sequences=False, activation='tanh')(gru_concat2)\n",
    "    #print(gru4.shape)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(1, activation='sigmoid')(gru4)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Assuming chrono_data_array.shape = (batch_size, sequence_length, num_features)\n",
    "model = ChronoNet(input_shape=(chrono_data_array.shape[1], chrono_data_array.shape[2]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974857dc-8c08-477f-a87e-5643a3d58e8d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### LOGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9f0c7-6598-49bd-9411-dfe078f15a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "best_model_path = 'best_5s_customcnn_gru_model_LOGO.h5'\n",
    "\n",
    "best_model_weights = None\n",
    "best_fold_info = None\n",
    "\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracies = []\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "confusion_matrices = []\n",
    "\n",
    "group_predictions = []\n",
    "actual_group_labels = []\n",
    "#shuffled_groups = shuffle_group_labels(one_d_cnn_group_array)\n",
    "\n",
    "\n",
    "# Setup TensorBoard callback\n",
    "log_dir = \"logs/duration/custom/LOGO\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "#shuffled_groups = shuffle_group_labels(chrono_group_array)\n",
    "for train_index, val_index in logo.split(chrono_data_array, chrono_label_array, groups=chrono_group_array):\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    print(\"Test group:\", np.unique(chrono_group_array[val_index]))\n",
    "    train_features,train_labels=chrono_data_array[train_index],chrono_label_array[train_index]\n",
    "    val_features,val_labels=chrono_data_array[val_index],chrono_label_array[val_index]\n",
    "\n",
    "    \n",
    "    scaler=StandardScaler()\n",
    "\n",
    "    train_features = scaler.fit_transform(train_features.reshape(-1, train_features.shape[-1])).reshape(train_features.shape)\n",
    "    val_features = scaler.transform(val_features.reshape(-1, val_features.shape[-1])).reshape(val_features.shape)\n",
    "    \n",
    "    model=gru_cnn_model()\n",
    "    \n",
    "    history = model.fit(train_features,train_labels,\n",
    "              epochs=10,\n",
    "              batch_size=128,\n",
    "              shuffle=True,\n",
    "              validation_data=(val_features,val_labels)\n",
    "             )\n",
    "    \n",
    "    # Append loss and accuracy from the training history\n",
    "\n",
    "    #print(history.history['val_loss'])\n",
    "    training_losses.append(history.history['loss'])\n",
    "    validation_losses.append(history.history['val_loss'])\n",
    "    training_accuracies.append(history.history['accuracy'])\n",
    "    validation_accuracies.append(history.history['val_accuracy'])\n",
    "    \n",
    "    raw_predictions = model.predict(val_features)\n",
    "    \n",
    "    group_prediction = np.mean(raw_predictions) > 0.5\n",
    "    print(\"Averaged Prediction: :\", np.mean(raw_predictions))\n",
    "    actual_group_label = np.mean(val_labels) > 0.5\n",
    "\n",
    "    group_predictions.append(group_prediction)  # True if average prediction > 0.5, False otherwise\n",
    "    actual_group_labels.append(actual_group_label)  # Assumes all labels in the group are the same\n",
    "\n",
    "    accuracies.append(((group_prediction == actual_group_label)*1).astype(int))\n",
    "    print(\"Correctly Identified: \",  group_prediction == actual_group_label)\n",
    "\n",
    "\n",
    "    if history.history['val_loss'][-1] < best_loss:\n",
    "        best_loss = history.history['val_loss'][-1]\n",
    "        # Save the model\n",
    "        #model.save(best_model_path)\n",
    "        #print(f\"Saved improved model with validation loss: {history.history['val_loss'][-1]}\")\n",
    "\n",
    "mean_training_losses = np.mean(training_losses,axis=0)\n",
    "mean_validation_losses = np.mean(validation_losses,axis=0)\n",
    "mean_training_accuracies = np.mean(training_accuracies,axis=0)\n",
    "mean_validation_accuracies = np.mean(validation_accuracies,axis=0)\n",
    "\n",
    "print(\"Final Validation Accuracy: \", np.mean(accuracies))\n",
    "      \n",
    "# Generate and store confusion matrix for the fold\n",
    "\n",
    "final_cm = confusion_matrix(actual_group_labels, group_predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(final_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Healthy', 'Schizophrenic'], yticklabels=['Healthy', 'Schizophrenic'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(actual_group_labels, group_predictions))\n",
    "ep = range(1,6)\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ep,mean_training_losses, label='Average Training Loss')\n",
    "plt.plot(ep,mean_validation_losses,label='Average Validation Loss')\n",
    "plt.title('Training and Validation Loss Across All Folds')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b992da8-1cef-4c3a-b882-676b692fe099",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_results(training_losses,\n",
    "                      training_accuracies,\n",
    "                      validation_losses, \n",
    "                      validation_accuracies,\n",
    "                      epochs=epochs,\n",
    "                      save_plots=True,\n",
    "                      save_dir='plots/5sec/customcnn_gru/logo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641efa06-3a1e-4d02-97d7-4160776057a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4084a6-2359-4252-842f-15a2e2dcb502",
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf=GroupKFold(n_splits=5)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracies = []\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model_path = 'best_customcnn_gru_GroupKFold.h5'\n",
    "\n",
    "best_model_weights = None\n",
    "best_fold_info = None\n",
    "\n",
    "# Setup TensorBoard callback\n",
    "log_dir = \"logs/duration/custom/GroupKFold\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "shuffled_groups = shuffle_group_labels(chrono_group_array)\n",
    "\n",
    "for train_index, val_index in gkf.split(chrono_data_array, chrono_label_array, groups=shuffled_groups):\n",
    "    train_features,train_labels=chrono_data_array[train_index],chrono_label_array[train_index]\n",
    "    val_features,val_labels=chrono_data_array[val_index],chrono_label_array[val_index]\n",
    "    \n",
    "    scaler=StandardScaler()\n",
    "\n",
    "    train_features = scaler.fit_transform(train_features.reshape(-1, train_features.shape[-1])).reshape(train_features.shape)\n",
    "    val_features = scaler.transform(val_features.reshape(-1, val_features.shape[-1])).reshape(val_features.shape)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model=gru_cnn_model()\n",
    "    \n",
    "    history = model.fit(train_features,train_labels,\n",
    "              epochs=10,\n",
    "              batch_size=128,\n",
    "              shuffle=True,\n",
    "              validation_data=(val_features,val_labels)\n",
    "             )\n",
    "    \n",
    "    # Append loss and accuracy from the training history\n",
    "    training_losses.append(history.history['loss'])\n",
    "    validation_losses.append(history.history['val_loss'])\n",
    "    training_accuracies.append(history.history['accuracy'])\n",
    "    validation_accuracies.append(history.history['val_accuracy'])\n",
    "\n",
    "    # Generate and store confusion matrix for the fold\n",
    "    raw_predictions = model.predict(val_features)\n",
    "    predictions = (raw_predictions > 0.5).astype(int)\n",
    "    cm = confusion_matrix(val_labels, predictions)\n",
    "\n",
    "\n",
    "    if history.history['val_loss'][-1] < best_loss:\n",
    "        best_loss = history.history['val_loss'][-1]\n",
    "        # Save the model\n",
    "        model.save(best_model_path)\n",
    "        print(f\"Saved improved model with validation loss: {history.history['val_loss'][-1]}\")\n",
    "\n",
    "    print(\"Final Validation Accuracy: \", np.mean(validation_accuracies,axis=0))\n",
    "    # Plotting the confusion matrix for each fold\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Healthy', 'Schizophrenic'], yticklabels=['Healthy', 'Schizophrenic'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e20c08-8283-48cb-a42b-6a81dac8ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_results(training_losses,\n",
    "                      training_accuracies,\n",
    "                      validation_losses, \n",
    "                      validation_accuracies,\n",
    "                      epochs=epochs,\n",
    "                      save_plots=True,\n",
    "                      save_dir='plots/5sec/customcnn_gru/gf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg",
   "language": "python",
   "name": "eeg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
